# Credit Risk Decision Engine (Production-Grade ML System)

**Probability of Default (PD) scoring + decision policy + audit-friendly reason codes**  
Built as a **production-grade ML system**: data contracts, evaluation-as-tests, versioned model bundle, FastAPI scoring service, Streamlit UI, monitoring + retraining runbook, and CI.

> âœ… This project is intentionally engineered like a real ML service youâ€™d ship:  
> **feature contract + reproducible artifacts + tests + monitoring + UI** (not a notebook demo).

---

## ğŸ–¼ï¸ Demo

### Streamlit UI (calls FastAPI `/score`)
![Streamlit UI](docs/diagrams/screenshots/ui.png)



---

## ğŸš€ What This System Does

Given a minimal set of applicant inputs (8â€“12 fields), the system returns:

- **PD** (calibrated probability of default)
- **Decision**: `approve` / `manual_review` / `decline` (policy thresholds stored in model metadata)
- **Reason codes**: lightweight, audit-friendly explanations (rule-based mapping from key features)
- **Model version + schema version + latency**
- **Prometheus metrics** at `/metrics`

---

## ğŸ§  Why This Is â€œProduction-Gradeâ€

This repo implements the engineering standards recruiters expect for real ML systems:

### âœ… Data & Schema Governance
- **Data contract tests** ensure:
  - required columns exist
  - primary key uniqueness
  - target validity (binary)
- Contract tests run in **CI** using a committed fixture sample (`data/fixtures/train_sample.parquet`)

### âœ… Leakage-Safe Modeling
- **Stratified split** with **ID disjointness checks** to prevent leakage across train/valid/test.

### âœ… Reproducible Model Bundle + Feature Contract
A trained model is packaged into a **versioned bundle** that includes:
- `model.joblib` (pipeline: preprocessing + classifier)
- `calibrator.joblib` (probability calibration)
- `metadata.json` (model version, schema version, thresholds, etc.)
- `reference_stats.json` (baseline stats used for monitoring)
- `feature_columns.json` (**the inference feature contract**)

This makes serving deterministic and prevents â€œtrain/inference mismatchâ€.

### âœ… Serving Layer (FastAPI)
- `/health` for readiness + bundle identity
- `/score` for inference (PD + decision + reason codes)
- `/metrics` for Prometheus scraping

### âœ… Monitoring + Retraining Runbook
Monitoring script produces a JSON report with:
- **PSI drift** on key features (EXT_SOURCE_2, AMT_INCOME_TOTAL, DAYS_BIRTH)
- **Calibration drift** via Brier score (when labels available)
- `recommended_action`: `ok` / `investigate` / `retrain_recommended`

### âœ… CI / Automation
GitHub Actions runs:
- install (via `pyproject.toml`)
- tests (including contract tests)
- inference contract tests

---

## ğŸ—ï¸ Architecture Overview

**Streamlit UI â†’ FastAPI API â†’ Bundle Loader â†’ Model + Calibrator**

- The UI sends user inputs to the API (recommended)
- The API owns inference via the bundle:
  - uses `feature_columns.json` to align inference schema
  - uses `calibrator.joblib` to output calibrated PD
  - uses decision policy stored in `metadata.json`

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ latest_eval.json
â”‚   â””â”€â”€ monitoring/  (generated monitor reports)
â”œâ”€â”€ bundle/
â”‚   â”œâ”€â”€ latest/PATH.txt
â”‚   â””â”€â”€ model_0.1.0/ (trained bundle artifacts)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ fixtures/train_sample.parquet   (CI fixture sample)
â”‚   â””â”€â”€ processed/train_table.parquet   (local training input; typically not committed)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ model_card.md
â”‚   â”œâ”€â”€ evaluation_report.md
â”‚   â”œâ”€â”€ ops_runbook.md
â”‚   â””â”€â”€ diagrams/screenshots/ui.png
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ monitor.py
â”œâ”€â”€ src/credit_risk_decision_engine/
â”‚   â”œâ”€â”€ serving/ (FastAPI app + loader + schemas)
â”‚   â”œâ”€â”€ modeling/ (bundle save/load, training)
â”‚   â”œâ”€â”€ monitoring/ (PSI utilities)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_contract.py
â”‚   â””â”€â”€ test_inference_contract.py
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ Makefile
â””â”€â”€ README.md


âœ… Quickstart (Local)
0) Create environment + install

python -m venv .venv
# Windows
.venv\Scripts\activate

python -m pip install -U pip
pip install -e ".[dev]"


ğŸ§ª Run Tests (Local)
python -m pytest -q
Note: tests that read local training data expect data/processed/train_table.parquet to exist locally.
CI uses data/fixtures/train_sample.parquet so it works without your full dataset.

ğŸ‹ï¸ Train + Build Bundle (Local)
Training produces:

artifacts/latest_eval.json

bundle/model_<version>/...

bundle/latest/PATH.txt


# Example (your project may already have a dedicated CLI/entrypoint)
python -c "from credit_risk_decision_engine.modeling.train import train_all; import pandas as pd; from credit_risk_decision_engine.config import SETTINGS; df=pd.read_parquet(SETTINGS.processed_data_dir/'train_table.parquet'); train_all(df)"
ğŸŒ Start the API (FastAPI)

python -m uvicorn credit_risk_decision_engine.serving.app:app --reload --host 0.0.0.0 --port 8000
Health check:



Invoke-WebRequest http://localhost:8000/health -UseBasicParsing
ğŸ–¥ï¸ Start the UI (Streamlit)
In a new terminal (keep API running):


streamlit run ui/streamlit_app.py
The UI will call:

http://localhost:8000/score

ğŸ” Monitoring (PSI + Brier Drift)
Run monitoring on a batch file:


python scripts/monitor.py data/fixtures/train_sample.parquet
Output includes PSI + Brier drift checks and a recommended action:


{
  "psi": {
    "EXT_SOURCE_2": {"psi": 0.0016, "status": "ok"},
    "AMT_INCOME_TOTAL": {"psi": 0.0021, "status": "ok"}
  },
  "brier": {
    "batch_brier": 0.0494,
    "baseline_brier": 0.0683,
    "brier_drift_flag": false
  },
  "recommended_action": "ok"
}
Monitoring reports are saved to:

artifacts/monitoring/monitor_report_<timestamp>.json

ğŸ“Š API Endpoints
GET /health â†’ bundle identity + schema version

POST /score â†’ PD + decision + reason codes

GET /metrics â†’ Prometheus metrics

Example request:


curl -X POST http://localhost:8000/score \
  -H "Content-Type: application/json" \
  -d '{
    "request_id": "demo-1",
    "features": {
      "EXT_SOURCE_1": 0.85,
      "EXT_SOURCE_2": 0.80,
      "EXT_SOURCE_3": 0.78,
      "AMT_INCOME_TOTAL": 150000,
      "AMT_CREDIT": 300000,
      "AMT_ANNUITY": 18000,
      "DAYS_BIRTH": -16000,
      "DAYS_EMPLOYED": -5000,
      "CODE_GENDER": "F",
      "FLAG_OWN_CAR": "Y",
      "NAME_EDUCATION_TYPE": "Higher education",
      "NAME_INCOME_TYPE": "Working"
    }
  }'
ğŸ§¾ Documentation
Architecture: docs/architecture.md

Model Card: docs/model_card.md

Evaluation Report: docs/evaluation_report.md

Ops Runbook (Monitoring + Retraining + Rollback): docs/ops_runbook.md

âš ï¸ Common Pitfalls (and How This Repo Avoids Them)
1) Train/Inference Schema Mismatch
âœ… Solved via feature_columns.json (the feature contract) and API-driven inference.

2) Data Leakage in Splits
âœ… Solved via ID disjointness checks + stratified split logic.

3) Uncalibrated Probabilities
âœ… Solved via probability calibration (stable PD output).

4) CI failures due to missing local datasets
âœ… Solved via committed fixture parquet used by contract tests in CI.

ğŸ§­ Roadmap (Optional Enhancements)
Dockerize API + UI with docker-compose

Add model registry workflow (MLflow â†’ stage promotion â†’ bundle export)

Add scheduled monitoring runs + alerting thresholds

Add rollback command: â€œswitch bundle/latest/PATH.txt to previous bundleâ€

ğŸ‘¤ Author
Built by Esther Uzor â€” AI/ML Engineer focused on production engineering, MLOps, and responsible deployment patterns.

ğŸ“œ License
MIT 







# 












